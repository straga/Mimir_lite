services:
  neo4j:
    image: neo4j:5.15-community
    container_name: neo4j_db
    ports:
      - "7474:7474"  # HTTP Browser UI
      - "7687:7687"  # Bolt protocol
    volumes:
      - ./data/neo4j:/data
      - ./logs/neo4j:/logs
      - ./data/neo4j/import:/var/lib/neo4j/import
    environment:
      - NEO4J_AUTH=neo4j/${NEO4J_PASSWORD:-password}
      - NEO4J_dbms_memory_pagecache_size=512M
      - NEO4J_dbms_memory_heap_initial__size=512M
      - NEO4J_dbms_memory_heap_max__size=2G
      - NEO4J_PLUGINS=["apoc"]
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "cypher-shell -u neo4j -p $${NEO4J_PASSWORD:-password} 'RETURN 1' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - mcp_network

  copilot-api:
    image: timothyswt/copilot-api-arm64:latest
    container_name: copilot_api_server
    ports:
      - "4141:4141"  # Fixed: copilot-api listens on 4141, not 3000
    volumes:
      - ./copilot-data:/root/.local/share/copilot-api  # Persist GitHub token
    environment:
      - NODE_ENV=production
      # Remove PORT=3000, the app uses 4141 by default
    restart: unless-stopped
    healthcheck:
      # Use CMD-SHELL so shell operators (||) work and allow a proper HTTP probe
      test: ["CMD-SHELL", "wget --spider -q http://localhost:4141/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 15s
    networks:
      - mcp_network

  ollama:
    build:
      context: ./docker/ollama
      dockerfile: Dockerfile
      args:
        - EMBEDDING_MODEL=${MIMIR_EMBEDDINGS_MODEL:-nomic-embed-text}
      tags:
        - mimir-ollama:${VERSION:-1.0.0}
        - mimir-ollama:latest
    image: mimir-ollama:${VERSION:-1.0.0}
    container_name: ollama_server
    ports:
      - "11434:11434"  # Ollama API
    # volumes:
      # - C:/Games/ollama:/root/.ollama  # Persist models from Windows path
    volumes:
      - type: bind
        source: ollama_models
        target: /root/.ollama
    environment:
      # Proxy settings (if needed in corporate environments)
      - HTTPS_PROXY=${HTTPS_PROXY:-}
      - HTTP_PROXY=${HTTP_PROXY:-}
      - NO_PROXY=${NO_PROXY:-}
      # Certificate settings (if you have custom CA certs)
      - SSL_CERT_FILE=${SSL_CERT_FILE:-}
      - CURL_CA_BUNDLE=${CURL_CA_BUNDLE:-}
      # Skip TLS verification for corporate proxies (use with caution)
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED:-0}
      # Ollama-specific settings
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_ORIGINS=*
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - mcp_network
    # Uncomment if you have GPU support (NVIDIA)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  mimir-server:
    build:
      context: .
      dockerfile: Dockerfile
      tags:
        - mimir-server:${VERSION:-1.0.0}
        - mimir-server:latest
    image: mimir-server:${VERSION:-1.0.0}
    container_name: mimir_server
    restart: unless-stopped
    environment:
      # Database Configuration
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=${NEO4J_PASSWORD:-password}
      
      # Server Configuration
      - NODE_ENV=production
      - PORT=3000
      
      # File Watching
      - FILE_WATCH_POLLING=true
      - FILE_WATCH_INTERVAL=1000
      - WORKSPACE_ROOT=/workspace
      - HOST_WORKSPACE_ROOT=${HOST_WORKSPACE_ROOT}  # Pass through from host
      
      # LLM Configuration
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      # - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
      - COPILOT_BASE_URL=${COPILOT_BASE_URL:-http://copilot-api:4141/v1}
      # - COPILOT_BASE_URL=${COPILOT_BASE_URL:-http://host.docker.internal:4141/v1}
      
      # Feature Flags (override .mimir/llm-config.json)
      - MIMIR_FEATURE_PM_MODEL_SUGGESTIONS=${MIMIR_FEATURE_PM_MODEL_SUGGESTIONS:-false}
      - MIMIR_FEATURE_VECTOR_EMBEDDINGS=${MIMIR_FEATURE_VECTOR_EMBEDDINGS:-true}
      
      # Embeddings Configuration (only used if VECTOR_EMBEDDINGS=true)
      - MIMIR_EMBEDDINGS_ENABLED=${MIMIR_EMBEDDINGS_ENABLED:-true}
      - MIMIR_EMBEDDINGS_PROVIDER=${MIMIR_EMBEDDINGS_PROVIDER:-ollama}
      - MIMIR_EMBEDDINGS_MODEL=${MIMIR_EMBEDDINGS_MODEL:-nomic-embed-text}
      - MIMIR_EMBEDDINGS_DIMENSIONS=${MIMIR_EMBEDDINGS_DIMENSIONS:-1024}
      - MIMIR_EMBEDDINGS_CHUNK_SIZE=${MIMIR_EMBEDDINGS_CHUNK_SIZE:-1024}
      - MIMIR_EMBEDDINGS_CHUNK_OVERLAP=${MIMIR_EMBEDDINGS_CHUNK_OVERLAP:-100}
      - MIMIR_EMBEDDINGS_DELAY_MS=${MIMIR_EMBEDDINGS_DELAY_MS:-1}
      - MIMIR_EMBEDDINGS_MAX_RETRIES=${MIMIR_EMBEDDINGS_MAX_RETRIES:-3}
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
      - ${HOST_WORKSPACE_ROOT:-~/src}:/workspace:ro
    ports:
      - "9042:3000"
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:3000/health', (res) => process.exit(res.statusCode === 200 ? 0 : 1)).on('error', () => process.exit(1))"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    depends_on:
      neo4j:
        condition: service_healthy
      copilot-api:
        condition: service_healthy
      # Ollama is required for embeddings but you can use a local server
      ollama:
        condition: service_healthy
    networks:
      - mcp_network

  # Open-WebUI with Mimir Pipeline Integration
  open-webui:
    build:
      context: .
      dockerfile: docker/open-webui/Dockerfile
      tags:
        - mimir-open-webui:${VERSION:-1.0.0}
        - mimir-open-webui:latest
    image: mimir-open-webui:${VERSION:-1.0.0}
    container_name: mimir-open-webui
    ports:
      - "3000:8080"
    volumes:
      - open-webui:/app/backend/data
    environment:
      # OpenAI-Compatible API Configuration (Copilot API)
      - OPENAI_API_BASE_URL=http://copilot-api:4141/v1
      # - OPENAI_API_BASE_URL=http://host.docker.internal:4141/v1
      - OPENAI_API_KEY=sk-copilot-dummy  # Dummy key for copilot-api
      
      # Disable Ollama (using Copilot API instead)
      - ENABLE_OLLAMA_API=false
      - OLLAMA_BASE_URL=
      
      # WebUI Configuration
      - WEBUI_NAME=Mimir
      - WEBUI_URL=http://localhost:3000
      - DEFAULT_MODELS=gpt-4.1  # Default selected model
      
      # Enable OpenAI API
      - ENABLE_OPENAI_API=true
    depends_on:
      - mimir-server
      - copilot-api
    networks:
      - mcp_network
    restart: unless-stopped
    extra_hosts:
      - "host.docker.internal:host-gateway"  # Allow access to host's localhost

volumes:
  open-webui:
  ollama_models:

networks:
  mcp_network:
    driver: bridge
