{
  "Lightweight LLMs": "Lightweight LLMs (1B–3B parameters) are optimized for local deployment, especially in resource-constrained environments such as Dockerized multi-agent frameworks. Top models include TinyLlama 1.1B (Llama 2 architecture, 3T tokens, ~637MB, 25.3% MMLU), Llama 3.2 1B/3B (vision+text, 128K context, up to 63.4% MMLU), Phi-3-mini 3.8B (69.7% MMLU, 128K context), Gemma 3 1B, and Qwen3 1.7B (multilingual). Strengths: small disk/memory footprint, fast inference (TinyLlama: ~60 tokens/sec CPU, ~120 tokens/sec GPU), commercial-friendly licenses, and broad quantization support. Weaknesses: lower reasoning and code quality than larger models, with performance varying by task. Unique features include support for long context windows (up to 128K), quantization, and multilingual capabilities (Qwen3). Typical use cases: code completion, simple reasoning, chat, and local RAG pipelines. [Source: LIGHTWEIGHT_LLM_RESEARCH.md]",
  "Embedding Models": "Nomic Embed Text v1.5 is the top lightweight embedding model, offering flexible dimensions (64–768), high performance (62.28 MTEB at 768d), and fast generation (~500 tokens/sec CPU). It supports task prefixes for search, clustering, and classification, and is compatible with Neo4j vector indexes. BGE models (bge-small-en-v1.5, bge-base-en-v1.5, bge-m3) are strong alternatives, especially for multilingual or hybrid search. Strengths: high quality for size, flexible dimension scaling, and broad integration support. Weaknesses: changing model or dimension requires full re-indexing. Unique features: Matryoshka Representation Learning (MRL) for dimension scaling. Use cases: semantic search, clustering, and classification in local or cloud vector databases. [Source: LIGHTWEIGHT_LLM_RESEARCH.md]",
  "Inference Frameworks": "Ollama is the primary inference framework, offering a Go-based, Docker-friendly, OpenAI-compatible API for LLMs and embeddings, with fast startup, model management, and CPU/GPU support. LocalAI is an alternative with broader backend support (llama.cpp, vLLM, Whisper, etc.), built-in WebUI, and P2P inference. vLLM is GPU-optimized for high-throughput, multi-GPU clusters but is less suited for single-model, Node.js/TypeScript environments. Strengths: easy deployment, broad model support, and integration with LangChain. Weaknesses: vLLM is complex for small-scale use, and LocalAI is less mature than Ollama. Unique features: Ollama’s automatic GGUF conversion and LocalAI’s federated inference. Use cases: local LLM inference, embeddings, and multi-agent orchestration. [Source: LIGHTWEIGHT_LLM_RESEARCH.md]",
  "Graph-RAG": "Graph-RAG enhances traditional Retrieval-Augmented Generation by leveraging knowledge graphs to provide contextual richness, explainability, and multi-hop reasoning. Unlike standard RAG, which retrieves isolated text chunks, Graph-RAG retrieves relationships between entities, enabling holistic context and transparent reasoning paths. Strengths: improved context management, explainability, and support for complex queries. Weaknesses: increased implementation complexity and the need for active context management to avoid issues like 'Lost in the Middle' and context poisoning. Unique features: subgraph extraction, contextual TODO enrichment, and hierarchical memory tiers. Typical use cases: enterprise knowledge management, complex task orchestration, and compliance/audit scenarios. All claims are directly supported by the cited research notes. [Source: GRAPH_RAG_RESEARCH.md]",
  "SWE-grep": "SWE-grep is a specialized, RL-trained subagent designed for fast, parallel context retrieval in coding agents. It operates by issuing multiple parallel tool calls (grep, glob, read) to quickly identify relevant files and line ranges, returning results in under 5 seconds. Strengths: speed, verifiable outputs, and context isolation for the main agent. Weaknesses: limited to retrieval tasks, requires custom model training, and less flexible than prompt-based orchestration. Unique features: hard turn budget (4 turns), weighted F1 scoring (precision > recall), and the 'flow window' concept for synchronous workflows. Typical use cases: rapid codebase search and context retrieval as a pre-processing step for larger agentic workflows. All claims are directly supported by the cited research notes. [Source: SWE_GREP_COMPARISON.md]",
  "Copilot-API": "Copilot-API is a reverse-engineered proxy that exposes GitHub Copilot’s API as an OpenAI-compatible endpoint, enabling integration with agentic frameworks. It requires a GitHub Copilot subscription and authentication via the gh CLI. Strengths: access to premium models (GPT-4o, Claude Opus), drop-in OpenAI API compatibility, and support for function calling. Weaknesses: requires paid subscription, authentication friction, rate limits, and is not officially supported by GitHub. Unique features: enables use of Copilot models in local agentic workflows and can be configured as an optional premium provider alongside local inference (Ollama). Typical use cases: premium LLM inference for planning, code generation, and validation in agentic systems. All claims are directly supported by the cited research notes. [Source: COPILOT_API_VS_OLLAMA_ANALYSIS.md]",
  "Vector Databases": "Pinecone, Weaviate, and Qdrant are leading vector databases for AI applications. Pinecone is fully managed, cloud-native, and optimized for scalability and low-latency search, but is cloud-only. Weaviate is open-source, supports hybrid search, and offers both managed and self-hosted options. Qdrant is open-source, supports distributed deployments, and is optimized for both CPU and GPU. Strengths: scalability, high performance, and broad integration (REST, Python, JS, LangChain, etc.). Weaknesses: Pinecone lacks on-premises deployment; operational complexity varies for self-hosted options. Unique features: Weaviate’s hybrid search and plugin system, Qdrant’s flexible deployment. Use cases: semantic search, RAG, and AI-powered applications requiring fast, scalable vector search. [Source: vector_db_research_notes.md]"
}