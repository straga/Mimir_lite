services:
  neo4j:
    image: neo4j:5.15-community
    container_name: neo4j_db
    ports:
      - "7474:7474"  # HTTP Browser UI
      - "7687:7687"  # Bolt protocol
    volumes:
      - ./data/neo4j:/data
      - ./logs/neo4j:/logs
      - ./data/neo4j/import:/var/lib/neo4j/import
    environment:
      - NEO4J_AUTH=neo4j/${NEO4J_PASSWORD:-password}
      - NEO4J_dbms_memory_pagecache_size=512M
      - NEO4J_dbms_memory_heap_initial__size=512M
      - NEO4J_dbms_memory_heap_max__size=2G
      - NEO4J_PLUGINS=["apoc"]
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "cypher-shell -u neo4j -p $${NEO4J_PASSWORD:-password} 'RETURN 1' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - mcp_network

  copilot-api:
    image: timothyswt/copilot-api-arm64:latest
    container_name: copilot_api_server
    ports:
      - "4141:4141"  # Fixed: copilot-api listens on 4141, not 3000
    volumes:
      - ./copilot-data:/root/.local/share/copilot-api  # Persist GitHub token
    environment:
      - NODE_ENV=production
      # Remove PORT=3000, the app uses 4141 by default
    restart: unless-stopped
    healthcheck:
      # Use CMD-SHELL so shell operators (||) work and allow a proper HTTP probe
      test: ["CMD-SHELL", "wget --spider -q http://localhost:4141/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 15s
    networks:
      - mcp_network

# Ollama (Legacy - kept for reference, can be removed)
  # ollama:
  #   build:
  #     context: ./docker/ollama
  #     dockerfile: Dockerfile
  #     args:
  #       - EMBEDDING_MODEL=${MIMIR_EMBEDDINGS_MODEL:-nomic-embed-text}
  #     tags:
  #       - mimir-ollama:${VERSION:-1.0.0}
  #       - mimir-ollama:latest
  #   image: mimir-ollama:${VERSION:-1.0.0}
  #   container_name: ollama_server
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - type: bind
  #       source: ollama_models
  #       target: /root/.ollama
  #   environment:
  #     - OLLAMA_HOST=0.0.0.0:11434
  #     - OLLAMA_ORIGINS=*
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD", "ollama", "list"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5
  #     start_period: 30s
  #   networks:
  #     - mcp_network
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]

  # llama.cpp server - OpenAI-compatible embeddings (ARM64 compatible)
  llama-server:
    image: timothyswt/llama-cpp-server-arm64-mxbai:latest
    container_name: llama_server
    ports:
      - "11434:8080"  # External 11434 -> Internal 8080 (llama.cpp default)
    # Note: Model is bundled in the image (nomic-embed-text, 261MB)
    # No volumes needed unless you want to add additional models
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - mcp_network
  #   # Uncomment if you have GPU support (NVIDIA)
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]

  mimir-server:
    build:
      context: .
      dockerfile: Dockerfile
      tags:
        - mimir-server:${VERSION:-1.0.0}
        - mimir-server:latest
    image: mimir-server:${VERSION:-1.0.0}
    container_name: mimir_server
    restart: unless-stopped
    environment:
      # Database Configuration
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=${NEO4J_PASSWORD:-password}
      
      # Server Configuration
      - NODE_ENV=production
      - PORT=3000
      
      # File Watching
      - FILE_WATCH_POLLING=true
      - FILE_WATCH_INTERVAL=1000
      - WORKSPACE_ROOT=/workspace
      - HOST_WORKSPACE_ROOT=${HOST_WORKSPACE_ROOT}  # Pass through from host
      
      # LLM Configuration
      # Using llama.cpp server with OpenAI-compatible API
      - OPENAI_API_BASE=http://llama-server:8080/v1
      - OPENAI_API_KEY=dummy
      - OLLAMA_BASE_URL=http://llama-server:8080/v1
      - COPILOT_BASE_URL=${COPILOT_BASE_URL:-http://copilot-api:4141/v1}
      # - COPILOT_BASE_URL=${COPILOT_BASE_URL:-http://host.docker.internal:4141/v1}
      
      # Provider and Model Configuration (100% dynamic - no config file needed)
      - MIMIR_DEFAULT_PROVIDER=${MIMIR_DEFAULT_PROVIDER:-copilot}
      - MIMIR_DEFAULT_MODEL=${MIMIR_DEFAULT_MODEL:-gpt-4.1}
      
      # Per-Agent Model Configuration (optional overrides)
      - MIMIR_PM_MODEL=${MIMIR_PM_MODEL:-}
      - MIMIR_WORKER_MODEL=${MIMIR_WORKER_MODEL:-}
      - MIMIR_QC_MODEL=${MIMIR_QC_MODEL:-}
      
      # Context Window Configuration (optional - system queries provider APIs dynamically)
      - MIMIR_DEFAULT_CONTEXT_WINDOW=${MIMIR_DEFAULT_CONTEXT_WINDOW:-128000}
      
      # Feature Flags (100% ENV-based, no config file needed)
      - MIMIR_FEATURE_PM_MODEL_SUGGESTIONS=${MIMIR_FEATURE_PM_MODEL_SUGGESTIONS:-false}
      - MIMIR_FEATURE_VECTOR_EMBEDDINGS=${MIMIR_FEATURE_VECTOR_EMBEDDINGS:-true}
      
      # Embeddings Configuration (only used if VECTOR_EMBEDDINGS=true)
      - MIMIR_EMBEDDINGS_ENABLED=true
      - MIMIR_EMBEDDINGS_PROVIDER=llama.cpp
      - MIMIR_EMBEDDINGS_MODEL=mxbai-embed-large
      - MIMIR_EMBEDDINGS_DIMENSIONS=${MIMIR_EMBEDDINGS_DIMENSIONS:-1024}
      - MIMIR_EMBEDDINGS_CHUNK_SIZE=${MIMIR_EMBEDDINGS_CHUNK_SIZE:-1024}
      - MIMIR_EMBEDDINGS_CHUNK_OVERLAP=${MIMIR_EMBEDDINGS_CHUNK_OVERLAP:-100}
      - MIMIR_EMBEDDINGS_DELAY_MS=${MIMIR_EMBEDDINGS_DELAY_MS:-0}
      - MIMIR_EMBEDDINGS_MAX_RETRIES=${MIMIR_EMBEDDINGS_MAX_RETRIES:-3}
      
      # Auto-index documentation on startup (default: true)
      # Allows users to immediately query Mimir's documentation via semantic search
      # Set to 'false' in .env to disable: MIMIR_AUTO_INDEX_DOCS=false
      - MIMIR_AUTO_INDEX_DOCS=${MIMIR_AUTO_INDEX_DOCS:-true}
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
      - ${HOST_WORKSPACE_ROOT:-~/src}:/workspace:ro
    ports:
      - "9042:3000"
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:3000/health', (res) => process.exit(res.statusCode === 200 ? 0 : 1)).on('error', () => process.exit(1))"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    depends_on:
      neo4j:
        condition: service_healthy
      copilot-api:
        condition: service_healthy
      llama-server:
        condition: service_healthy
    networks:
      - mcp_network

  # # Open-WebUI with Mimir Pipeline Integration
  # open-webui:
  #   build:
  #     context: .
  #     dockerfile: docker/open-webui/Dockerfile
  #     tags:
  #       - mimir-open-webui:${VERSION:-1.0.0}
  #       - mimir-open-webui:latest
  #   image: mimir-open-webui:${VERSION:-1.0.0}
  #   container_name: mimir-open-webui
  #   ports:
  #     - "3000:8080"
  #   volumes:
  #     - open-webui:/app/backend/data
  #   environment:
  #     # OpenAI-Compatible API Configuration (Copilot API)
  #     - OPENAI_API_BASE_URL=http://copilot-api:4141/v1
  #     # - OPENAI_API_BASE_URL=http://host.docker.internal:4141/v1
  #     - OPENAI_API_KEY=sk-copilot-dummy  # Dummy key for copilot-api
      
  #     # Disable Ollama (using Copilot API instead)
  #     - ENABLE_OLLAMA_API=false
  #     - OLLAMA_BASE_URL=
      
  #     # WebUI Configuration
  #     - WEBUI_NAME=Mimir Multi-Agent Orchestrator
  #     - WEBUI_URL=http://localhost:3000
  #     - DEFAULT_MODELS=gpt-4.1  # Default selected model
      
  #     # Enable OpenAI API
  #     - ENABLE_OPENAI_API=true
  #   depends_on:
  #     - mcp-server
  #     - copilot-api
  #   networks:
  #     - mcp_network
  #   restart: unless-stopped
  #   extra_hosts:
  #     - "host.docker.internal:host-gateway"  # Allow access to host's localhost

volumes:
  open-webui:
  ollama_models:

networks:
  mcp_network:
    driver: bridge
